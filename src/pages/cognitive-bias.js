
import React from "react"
import { Link } from "gatsby"

// Components
import Layout from "../components/layout"
import Hyperlink from "../components/Hyperlink"
import Image from "../components/image"
import Note from "../components/Note"
import SEO from "../components/seo"
import Section from "../components/Section"

// Images
import wikiCogBiasFull from "../images/wikipedia-cognitive-biases-full.png"
import wikiCogBiasZoom from "../images/wikipedia-cognitive-biases-zoom.png"

var notes = 0;
var sections = 0;
const CognitiveBias = () => (
  <Layout>
    <SEO title="Psychology of Cognitive Biases" />
    <blockquote>
    “<em>With remarkable ease, we form and sustain false beliefs. Led by our preconceptions, overconfident, persuaded by vivid anecdotes, perceiving correlations and control even where none may exist, we construct our social beliefs and then influence others to confirm them …
    <br /><br />
    [I]f anything, laboratory procedures overestimate our intuitive powers. The experiments usually present people with clear evidence and warn them that their reasoning ability is being tested. Seldom does real life say to us: ‘Here is some evidence. Now put on your intellectual Sunday best and answer these questions.’ … to cope with reality, we simplify it.</em>”<Note num={++notes}>David Myers (2013), <Hyperlink to="https://www.amazon.com/Social-Psychology-David-Myers/dp/0078035295"><em>Social Psychology</em></Hyperlink> pp. 114-115, "Social Beliefs and Judgments."</Note>
    </blockquote>
    The most consistent finding of psychology is that human thought and behavior are predictably irrational. No human, regardless of that human's intelligence, consistently forms accurate beliefs. Many people like to believe that they objectively and critically examine the information that they gain from experience, using it to form true beliefs. Without knowing it, though, humans usually form their beliefs for social reasons and actively deny the possibility of being wrong. The human mind is not optimized to discover and believe truth. Instead, it tends to delude itself.
    <br /><br />
    Common mistakes in reasoning due to human psychology are called <Hyperlink to="https://en.wikipedia.org/wiki/Cognitive_bias">cognitive biases</Hyperlink>, which are formally defined as "the systematic patterns of deviation from norm or rationality in judgment, whereby inferences about other people and situations may be drawn in an illogical fashion." The biggest problems with cognitive biases are how difficult they all are to overcome, how pervasively they affect everyone's reasoning, and how each person tends to ignore their influence on her reasoning.
    <br /><br />
    Below, I give an overview of cognitive biases and review research on some of the most important ones which mislead everyone into self-delusion. All of these should reduce your confidence in your own beliefs, and lead you to consider the possibility that many of them are unjustified or wrong.<Note num={++notes}>Most of the material in this section was adapted from an article that I posted online in May 2017: "<Hyperlink to="https://gregconan.wordpress.com/2017/05/22/our-irrational-minds-part-1/">Don’t Trust Your Beliefs: Our Irrational Minds, Part 1</Hyperlink>."</Note>
    <Section num={++sections} summary="Hundreds of cognitive biases distort everyone's thinking processes, making everyone think inaccurately and unreliably.">
        Hundreds of different biases make all humans irrational, as shown in <Hyperlink to="https://en.wikipedia.org/wiki/List_of_cognitive_biases">Wikipedia’s list of 185 cognitive biases</Hyperlink>. The image below is the most humbling that I have ever seen. Every line describes one distinct way that we often judge things incorrectly: 
        <br /><br />
        <img src = {wikiCogBiasFull} />
        <br />
        Here is a taste of the biases described by those lines:
        <br /><br />
        <img src = {wikiCogBiasZoom} />
        <br />
        All of those biases can be categorized based on what causes them, and the specifics of how they delude everyone:<Note num={++notes}>Also check out this <Hyperlink to="https://upload.wikimedia.org/wikipedia/commons/c/ce/Cognitive_Bias_Codex_With_Definitions,_an_Extension_of_the_work_of_John_Manoogian_by_Brian_Morrissette.jpg">poster including the name and definition of every bias</Hyperlink> in the 2017 "codex."</Note>
        <br /><br />
        <Hyperlink to="https://commons.wikimedia.org/wiki/File:The_Cognitive_Bias_Codex_-_180%2B_biases,_designed_by_John_Manoogian_III_(jm3).png"><img src = "https://upload.wikimedia.org/wikipedia/commons/a/a4/The_Cognitive_Bias_Codex_-_180%2B_biases%2C_designed_by_John_Manoogian_III_%28jm3%29.png" /></Hyperlink>
        <br />
        Yet it is not enough to simply show off how many biases there are. One must also realize how frequently they occur, and how much they tend to distort everyone's judgments. Fortunately, many researchers have examined the influence of cognitive biases.
    </Section>
    <Section num={++sections} summary="Many overconfidence biases cause everyone to overestimate the probability that they are right and underestimate the probability that they are wrong.">
        Because the causes of bias are usually unconscious, people rarely notice them. Consequently, <Hyperlink to="https://youtu.be/Qa_wJAS2cnI">confidence about one's knowledge is dangerously misleading</Hyperlink>. For example, the "<Hyperlink to="https://en.wikipedia.org/wiki/Illusory_superiority">illusory superiority effect</Hyperlink>" is "a cognitive bias whereby a person overestimates his or her own qualities and abilities, in relation to the same qualities and abilities of other persons." Variants include the Downing effect, "<Hyperlink to="https://en.wikipedia.org/wiki/Illusory_superiority#IQ">the tendency of people with a below-average IQ to overestimate their IQ, and of people with an above-average IQ to underestimate their IQ</Hyperlink>," and the <Hyperlink to="https://en.m.wikipedia.org/wiki/Dunning–Kruger_effect">Dunning-Kruger effect</Hyperlink>, "a cognitive bias wherein persons of low ability suffer from illusory superiority when they mistakenly assess their cognitive ability as greater than it is" because of the "metacognitive inability of low-ability persons to recognize their own ineptitude." Consider <Hyperlink to="https://en.wikipedia.org/wiki/Illusory_superiority#Cognitive_tasks">Dunning's & Kruger's initial study</Hyperlink>:
        <br /><br /><blockquote>
        "[P]articipants were given specific tasks (such as solving logic problems, analyzing grammar questions, and determining whether jokes were funny), and were asked to evaluate their performance on these tasks relative to the rest of the group, … [A]ll four groups evaluated their performance as above average, meaning that the lowest-scoring group (the bottom 25%) showed a very large illusory superiority bias. … [G]iven training, the worst subjects improved their estimate of their rank as well as getting better at the tasks."
        </blockquote>
        One encouraging result ostensibly seems to follow from research on the illusory superiority effect: it seems that people becoming more intelligent, educated, and/or self-aware will help them to overcome their biases. However, a 2012 University of Toronto study found that <Hyperlink to="http://www.newyorker.com/tech/frontal-cortex/why-smart-people-are-stupid">intelligence, education, and self-awareness did not help participants overcome cognitive biases. They even made the problem worse</Hyperlink> in some cases. Those traits correlated with a bigger “bias blind spot,” the assumption that other people are more susceptible to bias than oneself. The bias blind spot is caused by the ability to notice others’ bias but not one’s own, which is caused by having access to one’s own conscious thoughts but not those of others.
        <br /><br />
        A 2015 study asked people to rate their knowledge on a list of academic topics, including some nonexistent topics. They found that participants who claimed to know more about the real subjects, and those who actually knew more, <Hyperlink to="https://doi.org/10.1177/0956797615588195">claimed to know about topics that do not exist</Hyperlink> – a phenomenon called “overclaiming.” Even when participants were warned that some subjects were fake, they still overclaimed. Those who believe that they are smart enough to overcome cognitive biases could not be more wrong.
        <br /><br />
        Many meta-analyses have been conducted to estimate how much overconfidence distorts most people's knowledge of their own abilities. 22 meta-analyses including 354,739 participants total found that "<Hyperlink to="https://doi.org/10.1177/1745691613518075">the overall correspondence between self-evaluations of ability (e.g., academic ability, intelligence, language competence, medical skills, sports ability, and vocational skills) and objective performance measures (e.g., standardized test scores, grades, and supervisor evaluations) ... was .29</Hyperlink>.”<Note num={++notes}>Zell & Krizan (2014), "<Hyperlink to="https://doi.org/10.1177/1745691613518075">Do People Have Insight Into Their Abilities? A Metasynthesis</Hyperlink>" pp. 111 & 116, <Hyperlink to="https://journals.sagepub.com/doi/pdf/10.1177/1745691613518075">PDF</Hyperlink> pp. 1 & 6.</Note> Since the correlation between your abilities and your perceived abilities is less than 30%, including for your thinking abilities, you should be cautious and skeptical about your own thoughts.
    </Section>
    <Section num={++sections} summary="Confirmation bias makes everyone seek out evidence that they are right, and ignore evidence that they are wrong.">
        <blockquote>
        "<em>The human understanding when it has once adopted an opinion ... draws all things else to support and agree with it. And though there be a greater number and weight of instances to be found on the other side, yet these it either neglects and despises ... And such is the way of all superstitions, whether in astrology, dreams, omens, divine judgments, or the like; wherein men, having a delight in such vanities, mark the events where they are fulfilled, but where they fail, although this happened much oftener, neglect and pass them by</em>."
        <br />
        —Francis Bacon<Note num={++notes}>Francis Bacon (1620), <em>Novum Organum</em>, in E. A. Burtt, <em>The English Philosophers from Bacon to Mill</em> p. 36, as cited in Nickerson (1998), "<Hyperlink to="https://doi.org/10.1037%2F1089-2680.2.2.175">Confirmation Bias: A Ubiquitous Phenomenon in Many Guises</Hyperlink>" (<Hyperlink to="https://pdfs.semanticscholar.org/70c9/3e5e38a8176590f69c0491fd63ab2a9e67c4.pdf">PDF</Hyperlink>).</Note>
        </blockquote>
        Confirmation bias is the tendency to seek out or over-emphasize evidence confirming one's beliefs, and avoid or diminish evidence that contradicts one's beliefs. It is "perhaps the best known and most widely accepted notion of inferential error to come out of the literature on human reasoning,"<Note num={++notes}>Evans (1989), <em><Hyperlink to="https://psycnet.apa.org/record/1989-98394-000">Bias in Human Reasoning: Causes and Consequences</Hyperlink></em> p. 41, as cited by <Hyperlink to="https://doi.org/10.1037%2F1089-2680.2.2.175">Nickerson (1998)</Hyperlink>.</Note> and rightly so, because it has been called one of the "<Hyperlink to="https://doi.org/10.1037%2F1089-2680.2.2.175">single problematic aspect[s] of human reasoning that deserve attention above all others."</Hyperlink>
        <br /><br />
        Confirmation bias has <Hyperlink to="http://doi.org/10.1111/j.1751-9004.2007.00060.x">often been explained in terms</Hyperlink> of <Hyperlink to="https://en.wikipedia.org/wiki/Cognitive_dissonance">cognitive dissonance</Hyperlink>, "the mental discomfort (psychological stress) experienced by a person who holds two or more contradictory beliefs, ideas, or values." Someone who is exposed to information contradicting her beliefs will experience discomfort from cognitive dissonance. Everyone is then motivated to avoid information contradicting their prior beliefs, because exposure to that information causes distress — especially since few people are willing to immediately change their beliefs as soon as they encounter any information challenging those beliefs.
        <br /><br />
        <Hyperlink to="https://medium.com/@BruceLesley/trumps-border-wall-a-divisive-tool-to-hammer-bad-policy-18961358fd6a"><img src = "https://miro.medium.com/max/1200/1*Eje_pyCHgNhBChWxWZUfjw.jpeg" /></Hyperlink>
        <br />
        The "<Hyperlink to="http://rationalwiki.org/wiki/Backfire_effect">backfire effect</Hyperlink>" is a more extreme form of confirmation bias where <Hyperlink to="http://theoatmeal.com/comics/believe">seeing evidence for an idea that one disagrees with can make one believe that idea less</Hyperlink>. The human brain reacts to information challenging its worldview in the same way as it reacts to a physical threat, totally rejecting that information to protect its worldview. This bias is especially problematic for someone trying to learn the truth by examining ideas they disagree with.
        <br /><br />
        Here are a few examples of the research on confirmation bias and the backfire effect:
        <ul><br />
            <li>In a 1967 study, participants listened to a speech that was filled with static and could make it clearer by pressing a button. Overall, they used "selective attention" by <Hyperlink to="https://psycnet.apa.org/doi/10.1037/h0024088">ignoring information they disagreed with</Hyperlink>: smokers pressed the button more for a speech about why smoking is safe and nonsmokers pressed the button more for a speech about why smoking is unsafe.</li>
            <li>A 1979 Stanford University study made participants read two made-up studies about capital punishment, one calling it is effective and one calling it ineffective. Participants usually <Hyperlink to="https://psycnet.apa.org/doi/10.1037/0022-3514.37.11.2098">trusted the study that agreed with their opinion, but distrusted the study that did not</Hyperlink>. After seeing both studies, the participants' opinions were even more extreme than before.</li>
            <li>A 2017 study from Carnegie Mellon University found that participants tended to selectively remember information they agree with, <Hyperlink to="http://doi.org/10.1257/jel.20151245">selectively forget information they disagree with, interpret weak evidence as strong if it supports their beliefs</Hyperlink>, interpret strong evidence as weak if it contradicts their beliefs, and act defensive if they are shown a lot of information they disagree with.</li>
        </ul><br />
        Those studies make up the tip of the iceberg of psychological research on cognitive biases. Most of them I only found because they were used as examples in popular videos and articles about the topic. This 1998 review <Hyperlink to="https://doi.org/10.1037%2F1089-2680.2.2.175">summarizing the research on confirmation bias</Hyperlink>,<Note num={++notes}>The review has nine pages of references and has been <Hyperlink to="https://scholar.google.com/scholar?rlz=1C1CHFX_enUS654US654&um=1&ie=UTF-8&lr&cites=4886539840344743939">cited 4,670 times</Hyperlink> as of September 2019.</Note> for example, provides a more in-depth look. Fortunately, some have quantitatively summarized the research on how much people usually delude themselves with confirmation bias. A 2009 meta-analysis of 300 studies found a "<Hyperlink to="https://psycnet.apa.org/doi/10.1037/a0015701">moderate preference for congenial over uncongenial information (<em>d</em> = 0.36)</Hyperlink>"<Note num={++notes}>For reference, <Hyperlink to="https://en.wikipedia.org/wiki/Effect_size#Cohen's_d">Cohen's effect size (<em>d</em>)</Hyperlink> tends to range from 0.01 (very small) to 0.5 (medium) to 2 (huge).</Note><Note num={++notes}>Unfortunately, much of the research on confirmation bias and selective exposure to information "remains poorly integrated and somewhat isolated," according to Smith, Fabrigar, & Norris (2008), "<Hyperlink to="https://doi.org/10.1111/j.1751-9004.2007.00060.x">Reflecting on Six Decades of Selective Exposure Research: Progress, Challenges, and Opportunities</Hyperlink>" p. 477 (<Hyperlink to="https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1751-9004.2007.00060.x?casa_token=EC0s5h8E7ygAAAAA:HG2M0ZRIenYwWn99sXN2YhQ8x8wVETs8TKoTyakPAuCmd1b9zTqDVij7IcsVmr7QclBDwy-SHSs2De8">PDF</Hyperlink>). The <Hyperlink to="https://doi.org/10.1037/a0015701">Hart et al. (2009) meta-analysis</Hyperlink> has been the only one I could find so far.</Note> across the board. Even though there was some disagreement in the psychology literature about people selectively seeking exposure to information that confirms their beliefs, "<Hyperlink to="https://doi.org/10.1111/j.1751-9004.2007.00060.x">the existence of selective exposure effects is no longer seriously questioned, and the conditions that produce them are extensively documented."</Hyperlink><Note num={++notes}>Smith, Fabrigar, & Norris (2008) p. 478.</Note>
    </Section>
    <Section num={++sections} summary="The binary bias makes everyone force many of their perceptions into known categories, and either distort or ignore perceptions which do not fit those categories.">
        It is common for different categorizations to influence the way we think about and perceive reality. As an example, consider first that different languages tend to categorize things differently. As Tom Scott described, <Hyperlink to="https://youtu.be/2TtnD4jmCDQ">many different languages categorize colors differently</Hyperlink>: one language has one word for green and blue, whereas another language has two words for blue. With that in mind, consider this:
        <br /><br /><ol>
            <li>Language A and Language B categorize things 1 and 2 differently.</li>
            <li>Putting things into different categories changes how one sees and thinks about those things.</li>
            <li>So, thinking in Language A will make someone see and think about things 1 and 2 differently than thinking in Language B.</li>
        </ol>
        Unfortunately, psychology research provides plenty of evidence for point 2. I will describe some of it in detail below. But to summarize all of that research in two basic points:
        <br /><br /><ol>
            <li>Putting things into the same mental category makes someone see, and think about, them as more similar than they really are.</li>
            <li>Putting things into different mental categories makes someone see, and think about, them as more different than they really are.</li>
        </ol>
        Together, those tendencies compose "<Hyperlink to="https://doi.org/10.1177/0956797618792256">The Binary Bias</Hyperlink>," the "tendency to impose categorical distinctions on continuous data." In other words, if one thinks of information as a color spectrum, humans usually chop it in half and understand the spectrum as one half versus the other. A recent series of six studies with 1,851 participants found that "[a]cross a wide variety of contexts...when summarizing evidence, people exhibit a binary bias:"<Note num={++notes}>Fisher & Keil (2018), "<Hyperlink to="https://doi.org/10.1177/0956797618792256">The Binary Bias: A Systematic Distortion in the Integration of Information</Hyperlink>" (<Hyperlink to="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1693445/pdf/15590619.pdf">PDF</Hyperlink>).</Note> "<Hyperlink to="https://doi.org/10.1098/rstb.2004.1547">we tend to underestimate the difference between two facts...given the same categorical label, while we overestimate the difference between the same two facts…given different [ones]."</Hyperlink><Note num={++notes}>Robert M. Sapolsky (2004), "<Hyperlink to="https://doi.org/10.1098/rstb.2004.1547">The Frontal Cortex and the Criminal Justice System</Hyperlink>" p. 1788 (<Hyperlink to="http://faculty.missouri.edu/segerti/capstone/PFC&Criminaljustice.pdf">PDF</Hyperlink>).</Note>
        <br /><br />
        The Binary Bias is so hard-coded in the brain that it even exists on the level of specific neurons: In one study, some neurons would fire on seeing "the image of a dog or a cat (but not both)." The experimenters used Photoshop to gradually morph an image from a cat to a dog. The neurons fired consistently until the image was 50% cat 50% dog, then suddenly switched: "In other words, a neuron ‘considered’ a 60% dog to have more in common with a 100% dog than with a 40% dog."<Note num={++notes}>Sapolsky (2004), "<Hyperlink to="https://doi.org/10.1098/rstb.2004.1547">The Frontal Cortex and the Criminal Justice System</Hyperlink>" p. 1789.</Note>
        <br /><br />
        Human mental category systems often prevent people from seeing particular things, as shown by several psychology experiments. For example, a group of people were shown a series of playing cards in quick succession including a few "anomalous cards" like "a red six of spades and a black four of hearts." Participants almost always categorized the anomalous cards with the normal cards: "Without any awareness of trouble, it was immediately fitted to one of the conceptual categories prepared by prior experience. One would not even like to say that the subjects had seen something different than what they identified." After the length of time where participants saw the cards was increased (up to 40x slower), most picked out the anomalous cards — but 10% never did, much to their own personal distress. One said, "I can't make the suit out, whatever it is. It didn't even look like a card that time. I don't know what color it is now or whether it's a spade or a heart. I'm not even sure now what a spade looks like. My God!"<Note num={++notes}>These examples come from Thomas Kuhn, <em>The Structure of Scientific Revolutions</em> pp. 62-64.</Note>
        <br /><br />
        At first, most people forced their observations to match the categories they had learned. Once the observations were made more glaring, most of them could change their categories after a while, but some never could. 
        <br /><br />
        Is there any way to escape the conclusion that everyone is hopelessly deluded because their brains distort their thoughts and perceptions based on the categories that they have learned in our past experience? Maybe. The best that anyone can do is try to constantly break down the categories that they have learned into increasingly specific ones, and rely on those category boundaries as little as possible.
    </Section>
    </Layout>);

export default CognitiveBias;